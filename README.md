
***

# ğŸ¤– Conversational RAG Q&A System

A cutting-edge Retrieval-Augmented Generation (RAG) system for document Q&A with **live demo video**, AI-powered answers, source citations, and transparency â€“ powered by FastAPI, MongoDB Atlas, Qdrant Cloud, and Ollama LLM for optimal performance.

***

## ğŸš€ Live Demo & Visual Guide

### ğŸŒ [Live Demo Video](https://drive.google.com/file/d/1AmKZeT00FpON--W6ndOoyf9-xS8-pkL-/view?usp=sharing)

- See the system in action (full workflow, real context)  
- Hosted on RTX 5090 GPU for rapid AI inference

### ğŸ“¸ [Screenshot 1](https://drive.google.com/file/d/6ndOoyf9-xS8-pkL-/view?usp=sharing)  
### ğŸ“¸ [Screenshot 2](https://drive.google.com/file/d/6ndOoyf9-xS8)

- Main dashboard with document drag-and-drop and chatbot UI
- Real-time upload confirmation and chat response example

### ğŸ”— [GitHub Repository](https://github.com/LearnCodeWithRam/conversational_RAG_system.git)

***

## âœ¨ Features

- **Document Upload**: Seamlessly handle PDF & TXT files with automatic text extraction
- **Conversational Q&A**: Multi-turn intelligent chat powered by Ollama LLM
- **Source Citations**: Every answer is reference-backed for trust and transparency
- **Reasoning Traceability**: See precisely how responses are constructed from sources
- **Smart Suggestions**: Follow-up question prompts to deepen dialogue
- **Modern UI**: Drag-and-drop uploads, instant feedback, and live chat
- **Cloud Ready**: Built for MongoDB Atlas & Qdrant; scalable and robust
- **GPU Acceleration**: Harnesses RTX 5090 for fast inference

***

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend   â”‚<â”€â”€â”€â–ºâ”‚   FastAPI    â”‚<â”€â”€â”€â–ºâ”‚  Ollama LLM   â”‚
â”‚   (Web UI)  â”‚     â”‚    Server    â”‚     â”‚(llama3.1:8b)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
Â  Â  Â  Â  Â  Â  Â  Â  â”‚ Embeddings      â”‚
Â  Â  Â  Â  Â  Â  Â  Â  â”‚ (MiniLM, VLM)   â”‚
Â  Â  Â  Â  Â  Â  Â  Â  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” Â â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â” Â â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Â  Â â”‚  MongoDB   â”‚ Â â”‚ Qdrant Â â”‚ Â â”‚  Static â”‚
Â  Â â”‚ Â  Atlas Â  Â â”‚ Â â”‚ Vector Â â”‚ Â â”‚   UI Â  Â â”‚
Â  Â â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Â â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Â â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

***

## ğŸ“¸ User Interface Gallery

- Main dashboard: drag-and-drop upload area and chat interface
- Upload confirmation and chunking status
- Real-time Q&A with clickable source citations and follow-up prompts

***

## âš¡ï¸ Getting Started

### Prerequisites

- Python 3.11+
- MongoDB Atlas account
- Qdrant Cloud account
- RTX 5090 GPU (for full speed)
- Ollama server with llama3.1:8b model
- Docker (for container deployment)

### 1. Clone & Setup

```bash
git clone https://github.com/LearnCodeWithRam/conversational_RAG_system.git
cd conversational_RAG_system
pip install -r requirements.txt
```

### 2. Environment Configuration

Create a `.env` file (see sample below):

```env
DB_PASSWORD=
QDRANT_URL=
QDRANT_API_KEY=
MONGODB_URL=
CHUNK_SIZE=500
CHUNK_OVERLAP=80
MAX_CONTEXT_LENGTH=2000
```

### 3. Database Initialization

```bash
python setup_database.py
```

Tests all connections and sets up collections.

### 4. Download Embedding Model

```bash
python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('all-MiniLM-L6-v2').save('./all-MiniLM-L6-v2')"
```

### 5. Run Application

```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --reload  # Dev
# OR
uvicorn main:app --host 0.0.0.0 --port 8000           # Production
```

Open `http://localhost:8000` for the UI.

***

## ğŸ³ Docker Deployment

### Dockerfile

```dockerfile
FROM python:3.10-slim
WORKDIR /app
RUN apt-get update && apt-get install -y \
    gcc \
    poppler-utils \
    && rm -rf /var/lib/apt/lists/*
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
```

### Build & Run

```bash
docker build -t rag-system .
docker run -d --env-file .env -p 8000:8000 rag-system
```

#### For GPU:

```bash
docker run --gpus all --env-file .env -p 8000:8000 rag-system
```

***

## ğŸŒ Nginx Reverse Proxy (Production)

```nginx
server {
    listen 80;
    server_name your-domain.com;

    location / {
        proxy_pass http://127.0.0.1:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_read_timeout 300;
        proxy_connect_timeout 300;
        proxy_send_timeout 300;
    }
}
```

Restart nginx after configuration change.

***

## ğŸ—‚ï¸ Project Structure

```
RAG_SYSTEM/
â”œâ”€â”€ all-MiniLM-L6-v2/
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â””â”€â”€ pydantic_models.py
â”‚
â”œâ”€â”€ sampl_data/
â”‚   â”œâ”€â”€ colpali.pdf
â”‚   â””â”€â”€ inflation and economics trends.pdf
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ embeddings.py
â”‚   â””â”€â”€ ollama_service.py
â”‚
â”œâ”€â”€ static/
â”‚   â””â”€â”€ index.html
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â””â”€â”€ util_module.py
â”‚
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ config.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ main.py
â”œâ”€â”€ quick_start.md
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ setup_database.py
```

***

## ğŸ“š API Reference

Sample endpoints:

```bash
# Upload document
curl -X POST "http://your-domain.com/upload" -F "file=@document.pdf"

# Ask a question
curl -X POST "http://your-domain.com/ask" \
  -H "Content-Type: application/json" \
  -d '{ "user_id": "user123", "question": "What is AI?", "top_k": 4 }'

# Get history
curl -X GET "http://your-domain.com/history?user_id=user123"
```

***

## ğŸ› ï¸ Advanced (ColPali Integration Coming Soon)

A powerful update is in development to support **multimodal RAG**:  
- Incorporates document structure, tables, figures, and images
- Uses ColPali and ColQwen 2.5 for better layout and visual data understanding  
- Timeline: Rolling out in the next few days

References:  
- [ColPali Paper](https://arxiv.org/abs/2407.01449)  
- [ColPali GitHub](https://github.com/illuin-tech/colpali)  
- [ColQwen 2.5 Model](https://huggingface.co/vidore/colqwen2.5-v0.2)

***

## ğŸ Usage Tips

- Upload relevant PDFs or TXTs before asking
- Use specific questions for higher-accuracy results
- Verify info via clickable citations
- Clear system state before switching document sets

***






## ğŸ“š API Documentation

### Upload Document
```bash
curl -X POST "http://your-domain.com/upload" \
     -H "accept: application/json" \
     -H "Content-Type: multipart/form-data" \
     -F "file=@document.pdf"
```

### Ask Question
```bash
curl -X POST "http://your-domain.com/ask" \
     -H "Content-Type: application/json" \
     -d '{
       "user_id": "user123",
       "question": "What does the document say about AI?",
       "top_k": 4
     }'
```

### Get Conversation History
```bash
curl -X GET "http://your-domain.com/history?user_id=user123"
```

### List Documents
```bash
curl -X GET "http://your-domain.com/documents"
```

### Clear System
```bash
curl -X DELETE "http://your-domain.com/clear"
```

## ğŸ“± API Response Format

### Question Response
```json
{
  "answer": "Based on the uploaded documents, AI refers to...",
  "reasoning": "This answer was derived from section 2.1 of the uploaded report...",
  "references": [
    {
      "document": "AI_Report.pdf",
      "page": 2,
      "chunk_id": "chunk_123",
      "content_snippet": "Artificial Intelligence represents..."
    }
  ],
  "suggestions": [
    "What are the future implications of AI?",
    "How does AI impact different industries?",
    "What are the risks associated with AI?"
  ]
}
```

## ğŸ¯ Usage Process

### Step-by-Step Process

1. **Document Upload**
   - Navigate to the web interface
   - Drag and drop PDF/TXT files or click to browse
   - Wait for processing confirmation with chunk count

2. **Ask Questions**
   - Type your question in natural language
   - Press Enter or click "Ask" button
   - Receive detailed answer with reasoning

3. **Review References**
   - Check source citations for each answer
   - Click on references to see document snippets
   - Verify information accuracy

4. **Continue Conversation**
   - Use suggested follow-up questions
   - Ask related questions for deeper understanding
   - Build on previous context in multi-turn conversations

### Best Practices

- **Upload relevant documents** before asking questions
- **Ask specific questions** for better accuracy
- **Use follow-up suggestions** to explore topics thoroughly
- **Check references** to verify information
- **Clear system** when switching to different document sets

## ğŸ” Technical Configuration

### Chunking Parameters
```python
CHUNK_SIZE = 500          # Words per chunk
CHUNK_OVERLAP = 80        # Overlap between chunks
EMBEDDING_MODEL = "all-MiniLM-L6-v2"  # 384 dimensions
```

### LLM Settings
```python
MODEL = "llama3.1:8b"     # Ollama model
BASE_URL = "http://115.241.186.203"  # Ollama server
TEMPERATURE = 0.7         # Response creativity
TOP_K = 4                 # Context chunks per query
```

### Database Configuration
- **MongoDB**: Document metadata and conversation history
- **Qdrant**: Vector embeddings for semantic search
- **Vector Dimension**: 384 (matches embedding model)
- **Distance Metric**: Cosine similarity

## ğŸš¨ Troubleshooting

### Common Issues

1. **GPU Out of Memory**
   ```bash
   # Check GPU usage
   nvidia-smi
   
   # Restart Ollama service
   sudo systemctl restart ollama
   ```

2. **Database Connection Issues**
   ```bash
   # Test MongoDB connection
   python setup_database.py
   
   # Check network connectivity
   ping cluster0.gz3io9i.mongodb.net
   ```

3. **Ollama Service Issues**
   ```bash
   # Check Ollama status
   systemctl status ollama
   
   # View logs
   journalctl -u ollama -f
   
   # Test model
   ollama run llama3.1:8b "test"
   ```

4. **File Upload Issues**
   - Check Nginx client_max_body_size setting
   - Verify file permissions in upload directory
   - Ensure sufficient disk space

### Performance Optimization

1. **GPU Memory Management**
   - Monitor GPU usage with `nvidia-smi`
   - Adjust batch sizes for large documents
   - Use model quantization if needed

2. **Application Performance**
   - Increase number of workers for high traffic
   - Implement caching for frequent queries
   - Use connection pooling for databases

3. **Network Optimization**
   - Enable gzip compression in Nginx
   - Use CDN for static assets
   - Implement request rate limiting

## ğŸ“Š Deployment Status

### Live Demo
- **Deployment Method**: Docker + Nginx
- **GPU Server**: RTX 5090 powered
- **Model**: llama3.1:8b via Ollama
- **Status**: ğŸŸ¢ Deployed and Running

### Actual System Configuration
```
Deployment: Docker containerized application
Reverse Proxy: Nginx for global access
GPU: NVIDIA RTX 5090 (utilized via Docker GPU support)
LLM Backend: Ollama server (llama3.1:8b)
Databases: MongoDB Atlas + Qdrant Cloud
Embedding Model: all-MiniLM-L6-v2 (local)
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Make your changes and test thoroughly
4. Commit changes: `git commit -m 'Add amazing feature'`
5. Push to branch: `git push origin feature/amazing-feature`
6. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Ollama Team** for open-source LLM inference
- **MongoDB Atlas** for cloud database services
- **Qdrant** for vector database capabilities
- **Sentence Transformers** for embedding models
- **FastAPI** for the excellent web framework
- **NVIDIA** for RTX 5090 GPU performance

## ğŸ“ Support

For support and questions:
- **Issues**: Open a GitHub issue
- **Documentation**: Check this README and code comments
- **Community**: Join our Discord/Slack (if applicable)

---

**Built with â¤ï¸ for the AI community | Powered by RTX 5090 GPU**
